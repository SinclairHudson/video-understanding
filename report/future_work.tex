\section{Future work}

First and foremost, expanding to other datasets is critical to fully examine the generalization capability of the pipeline.
The current pipeline is currently very slow, requiring many forward passes of a large language model for every frame, and multiple frames for every clip.
It is experimental in nature, and has not been optimized for runtime performance.
With additional engineering effort, the performance of the video processing and text processing elements of the pipeline could both be greatly improved.
Future work could investigate further minimizing the number of frames processed, and prompting techniques to generate very dense captions, with a lot of information content in just a few tokens.

While this work focused on the visual features of video, audio is also often attached to video, and thus could be used to further improve the performance of the pipeline.
Related work such as VideoLLaMA has shown that audio can provide a small boost to performance \cite{videollama}.
Incorporating audio would be particularly important for understanding videos that have an emphasis on dialogue, such as movies.
If longer videos are to be processed, it would also be beneficial to explore more sophisticated clip partitioning algorithms, as mentioned in the discussion.
While not as important for performance on benchmark datasets, properly partitioning a long video into clips is a critical step for understanding videos in the real world.
Since the VideoDescriptor pipeline has 4 largely independent stages and many hyperparameters, there are many avenues for future work.
