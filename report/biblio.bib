@article{greenwade93,
    author  = "George D. Greenwade",
    title   = "The {C}omprehensive {T}ex {A}rchive {N}etwork ({CTAN})",
    year    = "1993",
    journal = "TUGBoat",
    volume  = "14",
    number  = "3",
    pages   = "342--351"
}

@article{clip4clip,
  title={Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning},
  author={Luo, Huaishao and Ji, Lei and Zhong, Ming and Chen, Yang and Lei, Wen and Duan, Nan and Li, Tianrui},
  journal={Neurocomputing},
  volume={508},
  pages={293--304},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{jsfusion,
  title={A joint sequence fusion model for video question answering and retrieval},
  author={Yu, Youngjae and Kim, Jongseok and Kim, Gunhee},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={471--487},
  year={2018}
}


@misc{xclip,
      title={X-CLIP: End-to-End Multi-grained Contrastive Learning for Video-Text Retrieval}, 
      author={Yiwei Ma and Guohai Xu and Xiaoshuai Sun and Ming Yan and Ji Zhang and Rongrong Ji},
      year={2022},
      eprint={2207.07285},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{internvideo,
      title={InternVideo: General Video Foundation Models via Generative and Discriminative Learning}, 
      author={Yi Wang and Kunchang Li and Yizhuo Li and Yinan He and Bingkun Huang and Zhiyu Zhao and Hongjie Zhang and Jilan Xu and Yi Liu and Zun Wang and Sen Xing and Guo Chen and Junting Pan and Jiashuo Yu and Yali Wang and Limin Wang and Yu Qiao},
      year={2022},
      eprint={2212.03191},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{bm25,
  title={Okapi at TREC-3},
  author={Robertson, Stephen E and Walker, Steve and Jones, Susan and Hancock-Beaulieu, Micheline M and Gatford, Mike and others},
  journal={Nist Special Publication Sp},
  volume={109},
  pages={109},
  year={1995},
  publisher={National Instiute of Standards \& Technology}
}

@article{llava,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2304.08485},
  year={2023}
}


@inproceedings{msr-vtt,
author = {Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong},
title = {MSR-VTT: A Large Video Description Dataset for Bridging Video and Language},
year = {2016},
month = {June},
abstract = {While there has been increasing interest in the task of describing video with natural language, current computer vision algorithms are still severely limited in terms of the variability and complexity of the videos and their associated language that they can recognize. This is in part due to the simplicity of current benchmarks, which mostly focus on specific fine-grained domains with limited videos and simple descriptions. While researchers have provided several benchmark datasets for image captioning, we are not aware of any large-scale video description dataset with comprehensive categories yet diverse video content.

In this paper we present MSR-VTT (standing for “MSR Video to Text”) which is a new large-scale video benchmark for video understanding, especially the emerging task of translating video to text. This is achieved by collecting 257 popular queries from a commercial video search engine, with 118 videos for each query. In its current version, MSR-VTT provides 10K web video clips with 41.2 hours and 200K clip-sentence pairs in total, covering the most comprehensive categories and diverse visual content, and representing the largest dataset in terms of sentence and vocabulary. Each clip is annotated with about 20 natural sentences by 1,327 AMT workers. We present a detailed analysis of MSR-VTT in comparison to a complete set of existing datasets, together with a summarization of different state-of-the-art video-to-text approaches. We also provide an extensive evaluation of these approaches on this dataset, showing that the hybrid Recurrent Neural Networkbased approach, which combines single-frame and motion representations with soft-attention pooling strategy, yields the best generalization capability on MSR-VTT.},
publisher = {IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)},
url = {https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video-and-language/},
}
