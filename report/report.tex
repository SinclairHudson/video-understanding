\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size \usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Video Query Processing with Text}
\author{Sinclair Hudson}

\begin{document}
\maketitle

\begin{abstract}
      This work explores different ways to leverage language models to convert video into textual descriptions, and then further answer queries about the video using the textual descriptions.
\end{abstract}

\section{Introduction}

Your introduction goes here! Simply start writing your document and use the Recompile button to view the updated PDF preview. Examples of commonly used commands and features are listed below, to help you get started.
We propose a video understanding pipeline using LLMs that can be used to...

\section{Related Work}

\subsection{Next Word Prediction}

\subsection{Video Foundation Models}

\subsection{Related Work}


CLIP4CLIP is a method that aims to extend the ideas of CLIP \cite{clip} to the video domain \cite{clip4clip}.
It generally follows a bi-encoder structure, in which the video and text are encoded using separate transformers \cite{transformer}, and then both representations are fed into a similarity calculator module.
The system is trained end-to-end with contrastive loss, as in CLIP \cite{clip}.

XCLIP \cite{xclip}

InternVideo is a recent attempt at a "foundation model" for video, being able to complete numerous downstream tasks.
The authors train InternVideo with a combination of multimodal constrastive learning (as in CLIP \cite{clip}), as well as masked video reconstruction, inspired by videoMAE \cite{videomae}.


\subsubsection{Multi-Modal Large Language Models}

\subsection{Video Retrieval}

\subsection{Video-Text datasets}
MSR-VTT (Microsoft Research Video to Text) is a popular video understanding dataset \cite{msr-vtt}.
This dataset can be used for multiple tasks, including Video Question Answering, Video Retrieval, and Video Captioning.
The dataset contains 10,000 videos, largely sourced from the internet (TODO verify).
For the video retrieval task, 1000 queries are used for testing, each specifying exactly one video as the correct answer \cite{jsfusion}.


\begin{figure}
      \centering
      \includegraphics[width=0.8\textwidth]{figures/msr-vtt-lengths.png}
      \caption{Length of videos in the MSR-VTT retrieval data split.}
      \label{fig:optical_flow}
\end{figure}

\section{Pipeline}

%TODO figure here
%\caption{The high level pipeline to use LLMs to understand video.}

For the purposes of this work, a "clip" is a small segment of a video that is a few seconds long, 
and is highly cohesive in what it depicts. For example, in a movie, a clip might be a single shot.
At a high level, the pipeline for video understanding using LLMs is as follows:
\begin{enumerate}
      \item Partition the video into individual clips (optional)
      \item From each clip, select a small subset of frames to represent the clip
      \item Using the selected frames, generate a textual description of the clip using an LLM.
      \item Using the textual description, answer queries about the video, or retrieve clips.
\end{enumerate}



\section{Clip Partitioning}

Videos can contain a lot of different clips, which may or may not be related.
The input for clip partitioning is the whole video, and the output is a list of breaks (frame numbers) between clips.
Uniform partitioning (every 10 seconds is a new clip)

\subsection{Selection based on L1}
The idea for selction based on L1 is to skip frames that are visually similar to the previously selected frame.
The selection strategy is greedy; the first frame is always selected, and then frames are considered in order, only selected if the frame differs from the previously selected frame by a certain L1 distance.
More formally

% TODO maybe just write an algorithm block for this? It's not difficult
\begin{equation}
      c < \frac{|I_{p} - I_{c}|}{W \times H \times C}
\end{equation}
where $c$ is a pre-defined threshold, $I_{p}$ is the previously selected frame, $I_{c}$ is the current frame, and $W$, $H$, $C$ are the width, height, and channels of the frames, respectively.

\begin{figure}
      \centering
      \includegraphics[width=0.8\textwidth]{figures/L1_frame_selection.png}
      \caption{Number of frames selected per video in the MSR-VTT dataset retrieval split, for different thresholds.}
      \label{fig:optical_flow}
\end{figure}

L1 was chosen over L2 because L2 is very sensitive to large changes in a few pixels; L1 gives a better sense of the overall change in the image.


\section{Frame Selection}

Even within a single clip, there are a lot of frames, most of which are very similar to their neighbours.
To make the pipeline efficient, it's critical to select the subset of the most semantically relevant frames.
LLMs can only effectively interpret one image at a time (todo verify).
The input to frame selection is a clip (small length of video) and the output is a list of frames selected for further processing.

Earlier works simply sample randomly \cite{TODO} or sample uniformly, at a course framerate such as 1 frame per second \cite{clip4clip}.

This work explores 3 different frame selection strategies:
greedy L1 selection, stratified sampling, and TODO.

\subsection{Greedy L1 Selection}
Greedy L1 selection initially selects the first frame of a clip. Then, it selects an additional clip if the L1 distance between the new frame and the previously selected frame is greater than some threshold (for example).
The L1 distance is normalized by the number of pixels in the image, so that the threshold is independent of the video resolution.
In the MSR-VTT dataset, most videos are quite short (TODO graph for this), and often only contain a single shot.
As such, when using Greedy L1 selection, a substantial number of videos only have the first frame selected (with L1 threshold 180).

\subsection{Stratified Sampling}
To get a potentially more diverse set of frames, the first, last and middle frames are selected.
We call this sampling method \verb|3-strat|. We also explore \verb|5-strat|, which selects the first, last, and 3 evenly spaced frames in between.

\section{Prompting Strategies}

The LLM used in testing this pipeline is LLava \cite{llava}.
LLaVA is selected because it is free and open source, and can be run on a single consumer GPU, making it amenable to experimentation.

Beam search, multiple prompts with a temperature.

For getting a textual description of a single image, the prompt used is "Please describe the objects in this image. Be as descriptive as possible.".
LLM performance can be sensitive to prompts.

The input to the LLM is a prompt and (potentially) multiple frames, and the output is a text caption.
In all cases, we limit the length of the caption to 512 words, to bound the amount of computation required per frame.

\section{Retrieval}

The last aspect of 

Because LLMs have described the content of the videos, the retrieval task is reduced to that of text retrieval, which is well-studied.

\section{Results}

\begin{table}[]
\begin{tabular}{|l|l|lll}
\cline{1-2}
                    & MSR-VTT V2T R@1 &  &  &  \\ \cline{1-2}
CLIP4Clip           & 45.6            &  &  &  \\ \cline{1-2}
XCLIP               & 49.3            &  &  &  \\ \cline{1-2}
InternVideo         & 55.2            &  &  &  \\ \cline{1-2}
3-strat-0-temp-BM25 & TODO            &  &  &  \\ \cline{1-2}
\end{tabular}
\caption{Results on MSR-VTT Text to Video retrieval task.}
\end{table}

\begin{table}[htbp]
  \centering
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Benchmark} & \multicolumn{3}{c}{\textbf{MSR-VTT}} \\
    \cmidrule(lr){2-4}
    & \textbf{R@1} & \textbf{B} & \textbf{C} \\
    \midrule
    \multirow{2}{*}{CLIP4Clip} & 0.85 & 0.88 & 0.90 \\
    & (0.02) & (0.01) & (0.03) \\
    \midrule
    \multirow{2}{*}{XCLIP} & 0.78 & 0.82 & 0.85 \\
    & (0.03) & (0.02) & (0.01) \\
    \midrule
    \multirow{2}{*}{InternVideo} & 0.92 & 0.89 & 0.91 \\
    & (0.01) & (0.02) & (0.01) \\
    \bottomrule
  \end{tabular}
  \label{tab:model_comparison}
  \caption{Performance Comparison of Models on Different Benchmarks}
\end{table}


\section{Future work}

Expanding methodologies to other datasets is critical to fully examine the generalization capability of these pipelines.
Additionally, this pipeline currently is very slow, requiring many forward passes of a large language model for every frame, and multiple frames for every clip.
Future work could investigate further minimizing the number of frames processed, and prompting techniques to generate very dense captions, with a lot of information content in few tokens.
Finally, while this work focused entirely on the visual features of video, audio is also often attached to video, and thus could be used to further improve the performance of the pipeline.

\section{Conclusion}

\bibliographystyle{alpha}
\bibliography{biblio}

\end{document}
