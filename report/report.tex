\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Video Query Processing with Text}
\author{Sinclair Hudson}

\begin{document}
\maketitle

\begin{abstract}
      This work explores different ways to leverage language models to convert video into textual descriptions, and then further answer queries about the video using the textual descriptions.
\end{abstract}

\section{Introduction}

Your introduction goes here! Simply start writing your document and use the Recompile button to view the updated PDF preview. Examples of commonly used commands and features are listed below, to help you get started.
We propose a video understanding pipeline using LLMs that can be used to...

\section{Related Work}

\subsection{Next Word Prediction}

\subsection{Video Foundation Models}

\subsection{Video Retrieval}

XCLIP \cite{}
CLIP4CLIP \cite{}
MSR-VTT (Microsoft Research Video to Text) is the dataset of choice, for video retrieval \cite{}.
This dataset can be used for multiple tasks, including Video Question Answering, Video Retrieval, and Video Captioning.

\section{Pipeline}

\section{Clip Partitioning}

Videos can contain a lot of different clips, which may or may not be related.
The input for clip partitioning is the whole video, and the output is a list of breaks (frame numbers) between clips.
Uniform partitioning (every 10 seconds is a new clip)


Selection based on L2

Selection based on Optical Flow.
The idea is to note places in the video in which there is a very large change (increase) in optical flow, indicating a scene change or at least a cut in the video.

\section{Frame Selection}

Even within a single clip, there are a lot of frames, most of which are very similar to their neighbours.
To make the pipeline efficient.
LLMs can only effectively interpret one image at a time.
The input to frame selection is a clip (small length of video) and the output is a list of frames selected for further processing.

\section{Prompting Strategies}

LLM performance can be sensitive to prompts.

Beam search, multiple prompts with a temperature.

The input to the LLM is a prompt and (potentially) multiple frames, and the output is a text caption.
In all cases, we limit the length of the caption to 200 words, to bound the amount of computation required frame.

\section{Retrieval}

The last aspect of 

Because LLMs have described the content of the videos, the retrieval task is reduced to that of text retrieval, which is well-studied.

\section{Testing Paradigms}

\section{Future work}

Expanding methodologies to other datasets is critical to fully examine the generalization capability of these pipelines.
Additionally, this pipeline currently is very slow, requiring many forward passes of a large language model for every frame, and multiple frames for every clip.
Future work could investigate further minimizing the number of frames processed, and prompting techniques to generate very dense captions, with a lot of information content in few tokens.
Finally, while this work focused entirely on the visual features of video, audio is also often attached to video, and thus could be used to further improve the performance of the pipeline.

\section{Conclusion}

\bibliographystyle{alpha}
\bibliography{biblio}

\end{document}
