\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

\usepackage[margin=1in]{geometry}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size \usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{tikz}
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;}

\title{VideoDescriptor: Video Understanding with Textual Descriptions}
\author{Sinclair Hudson}

\begin{document}
\maketitle

\begin{abstract}
      While LLMs are now frequently extended to process visual information in the form of images, they are not yet commonly used to process video.
      This work explores a general pipeline that leverages large language models (LLMs) to convert video into textual descriptions, and then further retrieve clips relevant to a query using those textual descriptions.
      The pipeline is called the VideoDescriptor pipeline, and is evaluated on text-to-video retrieval as well as video summarization.
      While not as accurate as other methods, the VideoDescriptor pipeline is able to achieve reasonable results on both tasks, and is completely zero-shot.
      Code for all experiments is available at \url{https://github.com/SinclairHudson/video-understanding}.
\end{abstract}

\input{introduction}

\input{related_work}

\input{method}

\input{results}

\input{discussion}

\input{future_work}

\section{Conclusion}

In this paper, the VideoDescriptor pipeline is proposed, consisting of 4 steps to process video into textual descriptions, and then retrieve clips relevant to a query.
While supervised methods still perform much better than the VideoDescriptor pipeline on text-to-video retrieval, the pipeline shows potential, especially for a zero-shot method.
Additionally, it can be used for video summarization, and generates compelling video summaries for queries relating to specific objects in the video.
The 4 main steps of the pipeline are all relatively simple, requiring no task-specific training, and can be improved upon or modified in future work.
Finally, since the VideoDescriptor pipeline is predominantly reliant on LLMs, it will likely benefit from future improvement of multimodal language models.

\bibliographystyle{alpha}
\bibliography{biblio}

\end{document}
