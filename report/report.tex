\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

\usepackage[margin=1in]{geometry}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size \usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{tikz}
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;}

\title{Video Query Processing with Text}
\author{Sinclair Hudson}

\begin{document}
\maketitle

\begin{abstract}
      While LLMs are now frequently extended to process visual information in the form of images, they are not yet commonly used to process video.
      This work explores a general pipeline that leverages large language models (LLMs) to convert video into textual descriptions, and then further answer queries about the video using the textual descriptions.
      This pipeline is called the VideoDescriptor pipeline, and is evaluated on video summariztion as well as text to video retrieval.
      While not as accurate as other methods, the VideoDescriptor pipeline is able to achieve reasonable results on both tasks, and is completely zero-shot.
\end{abstract}

\input{introduction}

\input{related_work}

\input{method}

\section{Results}

\subsection{Video Retrieval}

\begin{table}[htbp]
  \centering
  \begin{tabular}{lccccccc}
    \toprule
    \textbf{Approach} &FS & Prompt & Triple & \multicolumn{3}{c}{\textbf{MSR-VTT} T2V} \\
    \cmidrule(lr){5-7}
                      &&&& \textbf{R@1} & \textbf{R@5} & \textbf{R@10} \\
    \midrule
    CLIP4Clip \cite{clip4clip} &-&-&-& 0.445 & 0.714 &  0.816\\
    \midrule
    X-CLIP \cite{xclip} &-&-&-& 0.493 & 0.758 & 0.848 \\
    \midrule
    InternVideo \cite{internvideo} &-&-&-& 0.552 & - & - \\
    \bottomrule
    VideoDescriptor + BiEnc &5strat&C& & 0.293 & 0.492 & 0.492 \\
    \midrule
    VideoDescriptor + BM25 &5strat&T& & 0.176 & 0.364 & 0.446 \\
  \end{tabular}
  \label{tab:model_comparison}
  \caption{Performance comparison of the best VideoDescriptor configurations against other models on MSR-VTT T2V retrieval.}
\end{table}

Note that due to computational constraints, only the 1000 videos in the MSR-VTT retrieval split were indexed, while in related works the entire dataset of 10,000 videos are indexed.
As such, direct comparisons between the recall values between this work and related works should not be made.
The retrieval task for the proposed pipeline is much easier, since it is retrieving from a dataset one tenth the size.
Also note that this pipeline functions zero-shot, while all other models have been finetuned for text-to-video retrieval on MSR-VTT.


\begin{table}[htbp]
  \centering
  \begin{tabular}{lccccccc}
    \toprule
    \textbf{Approach} &FS & Prompt & Triple & \multicolumn{3}{c}{\textbf{MSR-VTT} T2V} \\
    \cmidrule(lr){5-7}
                      &&&& \textbf{R@1} & \textbf{R@5} & \textbf{R@10} \\
    \midrule
    VideoDescriptor + BiEnc &L1 greedy&V& & 0.214 & 0.415 & 0.415 \\
    \midrule
    VideoDescriptor + BiEnc &3strat&V& & 0.26 & 0.485 & 0.485 \\
    \midrule
    VideoDescriptor + BiEnc &3strat&C& & 0.276 & 0.468 & 0.468 \\
    \midrule
    VideoDescriptor + BiEnc &3strat&T& & 0.265 & 0.437 & 0.437 \\
    \midrule
    VideoDescriptor + BiEnc &3strat&V& \checkmark & 0.269 & 0.467 & 0.467 \\
    \midrule
    VideoDescriptor + BiEnc &3rand&V& & 0.281 & 0.457 & 0.457 \\
    \midrule
    VideoDescriptor + BiEnc &5strat&V& & 0.290 & 0.483 & 0.483 \\
    \midrule
    VideoDescriptor + BiEnc &5strat&C& & \textbf{0.293} & \textbf{0.492} & \textbf{0.492} \\
    \midrule
    VideoDescriptor + BiEnc &5strat&T& & 0.269 & 0.456 & 0.456 \\
    \midrule
    VideoDescriptor + BiEnc &5strat&V& \checkmark & 0.286 & 0.490 & 0.490 \\
    \midrule
    VideoDescriptor + BiEnc &5rand&V& & 0.272 & 0.440 & 0.440 \\
    \bottomrule

    VideoDescriptor + BM25 &L1 greedy&V& & 0.125 & 0.266 & 0.341 \\
    \midrule
    VideoDescriptor + BM25 &3strat&V& & 0.134 & 0.288 & 0.356 \\
    \midrule
    VideoDescriptor + BM25 &3strat&C& & 0.138 & 0.284 & 0.366 \\
    \midrule
    VideoDescriptor + BM25 &3strat&T& & 0.157 & 0.323 & 0.408 \\
    \midrule
    VideoDescriptor + BM25 &3strat&V& \checkmark & 0.122 & 0.277 & 0.347 \\
    \midrule
    VideoDescriptor + BM25 &3rand&V& & 0.142 & 0.280 & 0.340 \\
    \midrule
    VideoDescriptor + BM25 &5strat&V& & 0.167 & 0.326 & 0.411 \\
    \midrule
    VideoDescriptor + BM25 &5strat&C& & 0.150 & 0.331 & 0.417 \\
    \midrule
    VideoDescriptor + BM25 &5strat&T& & \textbf{0.176} & \textbf{0.364} & \textbf{0.446} \\
    \midrule
    VideoDescriptor + BM25 &5strat&V& \checkmark & 0.151 & 0.328 & 0.395 \\
    \midrule
    VideoDescriptor + BM25 &5rand&V& & 0.149 & 0.294 & 0.368 \\
    \midrule
  \end{tabular}
  \label{tab:video_descriptor_comparison}
  \caption{Performance of many VideoDescriptor configurations on MSR-VTT T2V retrieval}
\end{table}

\subsection{Qualitative Results on Video Summarization}
For video summarization, the task is to select a subset of clips from a video relevant to the query.
Sports games are desireable to summarize because they are long, with a few well-defined interesting events (goals, fouls, etc.).
To test this method, we will full sports games from YouTube.
In this full soccer game, 7 goals are scored, 4 by France and 3 by Argentina.

\begin{table}[htbp]
  \centering
  \begin{tabular}{lccccc}
    \toprule
    \textbf{Query} & \multicolumn{3}{c}{\textbf{Soccer}} & \multicolumn{3}{c}{\textbf{Hockey}}\\
    \cmidrule(lr){2-4}
    & \textbf{Recall} & Length \\
    \midrule
    \multirow{2}{*}{Goals Scored by team France, in dark blue jerseys.} & 0.85 & 0.88 & 0.90 & HR, HL, \\
    & (0.02) & (0.01) & (0.03) \\
    \midrule
    \multirow{2}{*}{Goals scored by team Argentina, in light blue and white jerseys.} & 0.78 & 0.82 & 0.85 \\
    & (0.03) & (0.02) & (0.01) \\
    \midrule
    \multirow{2}{*}{Goals scored} & 0.92 & 0.89 & 0.91 \\
    & (0.01) & (0.02) & (0.01) \\
    \bottomrule
  \end{tabular}
  \label{tab:video_summarization}
  \caption{Performance Comparison of Models on Different Benchmarks}
\end{table}

\input{discussion}

\section{Future work}

Expanding methodologies to other datasets is critical to fully examine the generalization capability of these pipelines.
Additionally, this pipeline is currently very slow, requiring many forward passes of a large language model for every frame, and multiple frames for every clip.
The current pipeline is experimental in nature, and has not been optimized for runtime performance.
With additional engineering effort, the performance of the video processing and text processing elements of the pipeline could both be greatly improved.
Future work could investigate further minimizing the number of frames processed, and prompting techniques to generate very dense captions, with a lot of information content in just a few tokens.

While this work focused on the visual features of video, audio is also often attached to video, and thus could be used to further improve the performance of the pipeline.
Related work such as VideoLLaMA has shown that audio can provide a small boost to performance \cite{videollama}.
Incoporating audio would be particularly important for understanding videos that have an emphasis on dialogue, such as movies.

Future work could also further investigate the influence of prompts and different LLMs on the performance of the pipeline.

\section{Conclusion}

While supervised methods still perform much better than the VideoDescriptor pipeline on text to video retrieval, the VideoDescriptor shows potential for zero-shot retrieval.
Hopefully, the performance of the VideoDescriptor pipeline improves as multimodal language models improve.

\bibliographystyle{alpha}
\bibliography{biblio}

\end{document}
