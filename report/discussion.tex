\section{Discussion}

\subsection{Clip Partitioning}

Qualitatively, this approach struggles when large objects move quickly in the video, resulting in large differences between frames.
Additionally, sudden changes in the video, like the flash of a camera, are also often falsely identified as a clip break due to the sudden difference in brightness.
Finally, slow transitions and fade transitions are often not detected because they gradually change from one clip to another.
Transitions found in traditional sports broadcasts can cause such issues.
Those are the 3 most common failure modes of the L1-based clip partitioning algorithm.
In general, false positives will increase the number of clips, and fragment true clips into multiple clips.
Some amount of false positives may be acceptable, depending on the application.
The increased number of clips may slow down the retrieval part of the pipeline, but fragmenting clips may not be problematic.
False negatives are potentially more damaging, creating a segment that is really two clips.
This could potentially unrelated clips into a downstream video summary, for example.

\subsection{Video Retrieval}

At a high level, it would seem that the Bi-Encoder performs better than BM25 for this retrieval task, especially for Recall@1.

For frame selection, it would appear that \verb|5strat| is slightly more performant when compared to equivalent \verb|3strat| experiments, indicating that the extra frames evenly spaced throughout the video are helpful for understanding the video.
The random sampling doesn't perform significantly worse than stratified sampling, contrary to expectation.
This could be due to the fact that the videos in MSR-VTT are relatively short, and so all sampling methods may return very siilar sets of frames for each video.
These videos often only contain one clip, so in this case random sampling isn't prone to catastrophically missing a scene in the video.

For prompt choice, it would seem that this task is relatively insensitive to prompts.
The differences between the concise (C) experiments and equivalent verbose (V) ones are negligible.
However, we do see a significant improvement for the stochastic temperature "prompt" (T), when using the BM25 retriever.
Intuitively this makes sense; a higher temperature will produce a more diverse set of words in the description, which will help tf-idf-based retrievers like BM25 find the right document.

Interestingly, the experiments that use "triple" frames don't perform notably better than the experiments that use single frames in video retrieval.
This seems to suggest that either the additional frames are redundant, or that LLaVA is not good at incorporating information from multiple images in a single generation step.
This is similar to the findings in CLIP4clip, where the authors find that processing frames in batches as 3D tensors doesn't improve performance over frame-by-frame processing \cite{clip4clip}.

The authors of VideoChat note that their similar VideoChat-Text pipeline struggled to understand "intricate temporal reasoning and causal inference".

One interesting feature about the retrieval results of the VideoDescriptor + DPR model is that recall@5 and recall@10 are always the same.
It would seem that if the correct video isn't in the top 5 results, it's not in the top 10 either.
The properties of the OpenAI embedding space aren't well understood, so it's difficult to say why this is the case.

