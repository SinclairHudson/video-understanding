\section{Discussion}

\subsection{Clip Partitioning}

Qualitatively, the proposed clip partitioning approach struggles when large objects move quickly in the video, resulting in large differences between frames.
Additionally, sudden changes in the video, like the flash of a camera, are also often falsely identified as a clip break due to the sudden difference in image brightness.
Finally, slow transitions and fade transitions are often not detected as clip breaks because they gradually change from one clip to another.
Animated transitions found in traditional sports broadcasts can cause such issues.
Those are the 3 most common failure modes of the L1-based clip partitioning algorithm.
In general, false positives will increase the number of clips, and fragment true clips into multiple clips.
While false positives will slow down the retrieval part of the pipeline, some amount of false positives may be acceptable, depending on the application.
False negatives are potentially more damaging, creating a segment that is really two clips.
This could potentially introduce unrelated clips into a downstream video summary, for example.
Using more sophisticated clip partitioning algorithms such as optical flow or scene change detection may help this part of the pipeline.

\subsection{Video Retrieval}
Below is a discussion of the results on the MSR-VTT retrieval dataset; relevant results can be found in Table \ref{tab:video_descriptor_comparison}.
First and foremost, it would seem that the Bi-Encoder retriever performs better than BM25 for this retrieval task, especially for Recall@1.
For frame selection, \verb|5strat| is slightly more performant when compared to equivalent \verb|3strat| experiments, indicating that the extra frames evenly spaced throughout the video are helpful for understanding the video.
The random sampling doesn't perform significantly worse than stratified selection, contrary to expectation. 
This could be due to the fact that the videos in MSR-VTT are relatively short, and so all frame selection methods may return very similar sets of frames for each video.
These videos often only contain one clip, so in this case random sampling isn't prone to catastrophically missing a scene in the video.

For prompt choice, it would seem that this task is relatively insensitive to prompts.
The differences between the concise (C) experiments and equivalent verbose (V) ones are very small, though the BM25 retriever seems to benefit from the verbose prompt, while the Bi-Encoder does better with the concise prompt.
We also see a significant improvement for the stochastic temperature prompting method (T), when using the BM25 retriever.
Intuitively this makes sense; a higher sampling temperature will produce a more diverse set of words in the description, which will help tf-idf-based retrievers like BM25 find the right document.

Interestingly, the experiments that use stratified triplet selection don't perform notably better than the experiments that use single frames in video retrieval.
This seems to suggest that either the additional frames are redundant, or that LLaVA is not good at incorporating information from multiple images in a single generation step.
This is similar to the findings in CLIP4clip, where the authors find that processing frames in batches as 3D tensors doesn't improve performance over frame-by-frame processing \cite{clip4clip}.
The authors of VideoChat also note that their similar VideoChat-Text pipeline struggled to understand ``intricate temporal reasoning and causal inference".



